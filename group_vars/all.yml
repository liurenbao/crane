---
## Kubernetes
## ********************************************************************************************************************************
# 部署 Kubernetes 对应版本, 不能建议修改, 如需修改请修改小版本进行部署测试, 大版本会引发部署参数不支持等问题.
k8s_version: 'v1.14.2'

# 部署 CNI 对应版本
cni_version: 'v0.7.5'

# 获取当前需要部署的 Kubernetes Master 总数量, 配置数量在 nodes 文件中获取
k8s_master_counts: "{{ groups['kube-master'] | length }}"

# 获取当前需要部署的 Kubernetes Node 总数量, 配置数量在 nodes 文件中获取
all_nodes_counts: "{{ groups['all'] | length }}"

# Kubernetes ApiServer 监听的端口号
k8s_master_apiservice_bind_port: 5443

# Kubernetes HaProxy 监听的端口号
# 在部署的 Kubernetes 集群架构中, 使用 HaProxy 作为 apiServer 负载均衡设备, 集群架构应为 SLB > HaProxy > apiServer
# 请自行设置端口号, 最好保持默认值
k8s_master_haproxy_bind_port: 6443

# Kubernetes 在集群中使用的 ClusterIP, 此地址依赖 k8s_cluster_ip_range 的网段
k8s_cluster_ip: '10.96.0.1'

# Kubernetes 集群使用的 ClusterIP 网段
k8s_cluster_ip_range: '10.96.0.0/12'

# Kubernetes ApiServer 的总入口地址, 可配置 SLB 等云服务商提供的地址, 但需要接入后端服务器的 80, 443 端口, 此地址可以写某个单实例的 IP 地址
k8s_load_balance_ip: 34.92.146.194

# Kubernetes HaProxy 监听的 apiServer 完整的 URL
k8s_apiserver_https_bind: 'https://{{ k8s_load_balance_ip }}:{{ k8s_master_haproxy_bind_port }}'

# Kubernetes 可暴露的 NodePort 端口号
k8s_apiserver_node_port_range: '70-32767'

# 判定值, 勿动
is_kube_master: "{{ inventory_hostname in groups['kube-master'] }}"
is_kube_node: "{{ inventory_hostname in groups['kube-node'] }}"

# 是否使用本地文件, 例如 kubectl, image 不建议开启此设置, 目前为测试状态
is_using_local_files_deploy: true

# 是否使用远程镜像部署, 例如 kubectl, image 默认为 true
# 如果使用镜像部署需要保证 Ansible 中 *_version 未被修改的情况下才可使用, 否则版本不一致造成的问题自行解决.
# 如果服务器是在国内, 强烈推荐使用此值设为 true, 并保证可以获取 docker hub 镜像资源.
is_using_image_deploy: false

# 是否开启 Master 节点调度, 默认为 true. 
is_kube_master_schedule: true

# infra image, 依赖下方镜像仓库地址
k8s_pod_infra_container_image: "{{ k8s_cluster_component_registry }}/pause:3.1"

# Kubernetes Cluster Registry Addr, 默认值为: k8s.gcr.io
# 此地址可以修改为自定义镜像地址, 但需要确保已经执行 script/PublishK8sRegistryImages.sh 脚本, 所以参数可改为 slzcc, 如果在国内可使用 docker.io/slzcc 获取镜像。
# 受影响的镜像为: etcd、kube-apiserver-amd64、kube-controller-manager、kube-scheduler、kube-proxy、pause。
# 此值依赖于 is_kube_master_schedule, 如果使用镜像方式部署时, 此值不能变动
k8s_cluster_component_registry: 'k8s.gcr.io'

## Docker
## ********************************************************************************************************************************

# 使用 https://get.docker.com 方式安装, 并默认使用 Aliyun 镜像源, 可设为空.
docker_install_source: "Aliyun"

# 镜像加速, 如果为空则不设置.
docker_registry_mirrors: '["https://registry.docker-cn.com"]'

# Docker 存储路径
docker_data_root: '/var/lib/docker'

# Docker Image 存储驱动
docker_storage_driver: "overlay2"

# 允许的不安全 Registry 地址
docker_insecure_registry: '[]'

# docker cgroup drive
docker_cgroup_drive: '{{ kubelet_cgroup_drive }}'

## HaProxy
## ********************************************************************************************************************************

# 配置 stats 的 URI 地址
haproxy_stats_url: '/haproxy_stats'

# 监听 stats 的端口
haproxy_stats_port: 9090

# 访问 stats 时所需的账户/密码
haproxy_admin_username: 'admin'
haproxy_admin_password: 'admin123'

# Haproxy Config Path
haproxy_etc_dirs: '/etc/haproxy/'

## Configuration Directory
## ********************************************************************************************************************************

# Kubectl DirName
kubernetes_ctl_path: '/usr/local/bin/'

# Kubernetes PKI Path
kubernetes_pki_dirs: '{{ kubernetes_etc_dirs }}pki/'

# Etcd Config Path
kubernetes_etc_dirs: '/etc/kubernetes/'

# Kubernetes Manifests Path
kubernetes_manifests_dirs: '{{ kubernetes_etc_dirs }}manifests/'

# CNI Bin Path
kubernetes_cni_dirs: '/opt/cni/bin'

# Kubectl 工作目录
kubelet_work_dirs: '/var/lib/kubelet/'

# 前置临时目录
temporary_dirs: '/tmp/'

# Cgroup Drive
kubelet_cgroup_drive: 'cgroupfs'

## Systemd Config
## ********************************************************************************************************************************

# Systemd 子配置文件目录
systemd_etc_dirs: '/etc/systemd/system/'

# Systemd 主配置文件目录
systemd_default_dirs: '/lib/systemd/system/'

## SSH
## ********************************************************************************************************************************

# 使用 Ansible 部署时连接实例的用户
ssh_connect_user: "shilei"

# 使用 Ansible 部署时连接实例的 SSH 端口
ssh_connect_port: 22

# 使用 Ansible 部署时, 如需要节点之间进行协作传入的证书地址
target_ssh_private_key_file: "{{ temporary_dirs }}.general-id_rsa"

# 执行 SSH 或者 SCP 的公钥地址
ssh_public_key: "{{ target_ssh_private_key_file }}"

# 本地的公钥/私钥地址
source_ssh_public_key_file: "{{ hostvars[inventory_hostname]['ansible_ssh_public_key_file'] }}"
source_ssh_private_key_file: "{{ hostvars[inventory_hostname]['ansible_ssh_private_key_file'] }}"

## Keepalived
## ********************************************************************************************************************************

# 是否使用 Keepalived, 在非公有云/私有云环境中, 需要借助 keepalived 进行服务集群的高可用, 它的位置在类 SLB 层
is_keepalived: false

# Keepalived 绑定的 IP 地址, 它在 HaProxy 的上一层
keepalived_ip: "{{ k8s_load_balance_ip }}"

# Keepalived 绑定的物理网卡名称
vip_bind_net_device: "{{ os_network_device_name }}"

## Etcd
## ********************************************************************************************************************************

# Etcd 绑定的宿主机网卡, 此配置需要在启动 Etcd 的节点上强制网卡名称一致
os_network_device_name: 'ens4'

# Etcd Verions
etcd_version: '3.3.10'

# Etcd 使用的 Client 协议, 默认 HTTPS, 不能修改
etcd_url_scheme: "https"

# Etcd 使用的 Cluster Peer 协议, 默认 HTTPS, 不能修改
etcd_peer_url_scheme: "https"

# Etcd Client TLS Auth
etcd_client_cert_auth: true

# Etcd Peer TLS Auth
etcd_peer_client_cert_auth: true

# Etcd Client Port
etcd_client_port: 2379

# Etcd Peer Port
etcd_peer_port: 2380

# Etcd Client TLS Auto
etcd_client_tls_auto: true

# Etcd Peer TLS Auto
etcd_peer_tls_auto: true

# Etcd Cluster Token
etcd_cluster_token: 'etcd-k8s-cluster'

# Etcd Cluster Type
etcd_cluster_type: 'new'

# 占位符
etcd_interface: ""

# 判定值, 勿动
is_etcd_master: "{{ inventory_hostname in groups['etcd'] }}"

# Etcd Certs Path
etcd_ssl_dirs: '/etc/etcd/ssl/'

# Etcd Data Path
etcd_data_dirs: '/var/lib/etcd/'

## Add Nodes
## ********************************************************************************************************************************

# Calico Pool CIDR
calico_ipv4_pool_cidr: '192.168.0.0/16'

calico_version: 'v3.7.2'

# CNI Config Dir
cni_config_dir: '/etc/cni/net.d'

# 判定值, 勿动
is_add_master: "{{ inventory_hostname in groups['k8s-cluster-add-master'] }}"
is_add_node: "{{ inventory_hostname in groups['k8s-cluster-add-node'] }}"

## Kubernetes Add Ons
## ********************************************************************************************************************************

# DNS 服务所需的 Domain 后缀
dns_domain: 'cluster.local'

# DNS 服务的 ClusterIP 地址, 依赖集群 ClusterIP 网段
dns_cluster_ip: '10.96.0.10'

# DNS 集群 ClusterIP 网段
dns_cluster_range: "{{ k8s_cluster_ip_range }}"

# CoreDNS Version
dns_version: '1.5.0'

# 部署 Helm
is_deploy_helm: false

# 部署 Ingress Nginx
is_deploy_ingress_nginx: false

# 部署 Ingress Nginx Example
is_deploy_ingress_nginx_example: false

# Ingress Nginx Example Domain
# Ingres 示例使用的域名, 如果没有自定义域名, 可配置本机 Hosts
ingress_nginx_example: 'www.example.com'

# 部署 Prometheus Operator
is_deploy_prometheus_operator: false

# 部署 Prometheus Operator Ingress 所使用的 Ingress Domain
monitoring_ingress_prometheus_domain: 'prometheus.example.com'
monitoring_ingress_grafana_domain: 'grafana.example.com'
monitoring_ingress_alertmanager_domain: 'alertmanager.example.com'

# 部署 DNS Example Tools
# 用来测试集群内是否 DNS 可用的 Tools Pods
is_deploy_busybox_example: false

# 部署 Istio
# 注意 部署 Istio 不建议与 Prometheus Operator 一同部署, 可能会引发服务之间的冲突问题, 如需一同部署请自行解决依赖
is_deploy_istio: false

# Istio Version
istio_version: '1.1.7'

# 部署 Istio 所使用的 Ingress Domain
istio_ingress_prometheus_domain: 'prometheus.example.com'
istio_ingress_grafana_domain: 'grafana.example.com'
istio_ingress_tracing_domain: 'tracing.example.com'
istio_ingress_kiali_domain: 'kiali.example.com'

# istio kiali user/pass base64 coding
istio_kiali_username: 'YWRtaW4='
istio_kiali_password: 'YWRtaW4='

## Clean Kubernetes Cluster
## ********************************************************************************************************************************

# 清除集群时是否删除 Docker
is_remove_docker_ce: false

# 清除 Docker CE 时清除数据目录
is_remove_docker_dir: false

# 清除集群时是否删除 CFSSL 可执行文件
is_remove_cfssl: true

# 清除集群时是否删除 CNI 插件
is_remove_cni: true
